{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a639664-173a-4120-92a5-e218426b5daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1359bc53-5185-4edd-9b5a-859a7a1deb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -qq install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb577d6d-1742-4251-a7a9-f8cc5f76809a",
   "metadata": {},
   "source": [
    "# Check Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ea0ad7-16dc-491e-a449-e07e94f58e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am I in Colab? False\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "print(f\"am I in Colab? {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3892cfc-5310-45c4-bef9-0ec459cdd633",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e425997c-6b92-4223-9451-c042fbd7c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "# Elasticsearch\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd858d1-32a7-41eb-80f7-64bf58b7d633",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2158542-6573-48aa-8af3-0de64f59f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  # Colab\n",
    "  BASE_DIR = \".\"\n",
    "  BACKUPS_DATA_DIR   = \"/content/drive/MyDrive/Colab Notebooks/Dataclub/llm/data\"\n",
    "else:\n",
    "  # Local\n",
    "  BASE_DIR = \"..\"\n",
    "  BACKUPS_DATA_DIR   = \"../backups\"\n",
    "\n",
    "# Raw directory\n",
    "RAW_DATA_DIR               = f\"{BASE_DIR}/data/raw\"\n",
    "RAW_DOCS_DATA_DIR          = f\"{BASE_DIR}/data/raw/documents\"\n",
    "RAW_INFO_DATA_DIR          = f\"{BASE_DIR}/data/raw/info\"\n",
    "# Preprocessing\n",
    "PROCESSED_DATA_DIR         = f\"{BASE_DIR}/data/processed\"\n",
    "PROCESSED_DOCS_DATA_DIR    = f\"{BASE_DIR}/data/processed/documents\"\n",
    "# Indexing\n",
    "INDEXING_DATA_DIR          = f\"{BASE_DIR}/data/indexing\"\n",
    "INDEXING_DOCS_DATA_DIR     = f\"{BASE_DIR}/data/indexing/documents\"\n",
    "# Test directory\n",
    "TEST_DATA_DIR               = f\"{BASE_DIR}/data/test\"\n",
    "TEST_OPTIMIZATION_DATA_DIR  = f\"{BASE_DIR}/data/test/optimization\"\n",
    "GROUND_TRUTH_DATA_DIR       = f\"{BASE_DIR}/data/test/ground_truth\"\n",
    "GROUND_TRUTH_DOCS_DATA_DIR  = f\"{BASE_DIR}/data/test/ground_truth/documents\"\n",
    "GROUND_TRUTH_GEN_DATA_DIR   = f\"{BASE_DIR}/data/test/ground_truth/generated\"\n",
    "# Config Prompts\n",
    "PROMPTS_CONFIG_DIR = f\"{BASE_DIR}/cooking_recipe_assistant/config/prompts\"\n",
    "\n",
    "# Raw Info\n",
    "PLAYLIST_INFO_PATH = f\"{RAW_INFO_DATA_DIR}/playlist_info.pkl\"\n",
    "VIDEO_PLAYLIST_MAP_PATH = f\"{RAW_INFO_DATA_DIR}/video_playlist_map.pkl\"\n",
    "\n",
    "# Ground-truth\n",
    "GROUND_TRUTH_PATH = f\"{GROUND_TRUTH_DATA_DIR}/ground-truth-retrieval.csv\"\n",
    "\n",
    "# Optimization\n",
    "REST_OPT_ES_BM25_PATH       = f\"{TEST_OPTIMIZATION_DATA_DIR}/res-opt-es-bm25.json\"\n",
    "REST_OPT_ES_HYBRID_PATH     = f\"{TEST_OPTIMIZATION_DATA_DIR}/res-opt-es-hybrid.json\"\n",
    "REST_OPT_ES_HYBRID_RRF_PATH = f\"{TEST_OPTIMIZATION_DATA_DIR}/res-opt-es-hybrid-rrf.json\"\n",
    "\n",
    "# Make dirs if not exists\n",
    "if not os.path.exists(RAW_DATA_DIR):\n",
    "  print(\"Not exists dir: \", RAW_DATA_DIR)\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_OPTIMIZATION_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(GROUND_TRUTH_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(BACKUPS_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa593c3f-7b54-4bed-b47f-ae90671a085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Agregar solo si no está ya en sys.path\n",
    "if BASE_DIR not in sys.path:\n",
    "    sys.path.append(BASE_DIR)\n",
    "\n",
    "# Ahora puedes importar tu módulo\n",
    "import cooking_recipe_assistant\n",
    "from cooking_recipe_assistant.commons.utils import (\n",
    "    read_pickle, \n",
    "    read_document, \n",
    "    read_text,\n",
    "    save_pickle, \n",
    "    save_document,\n",
    "    save_text\n",
    ")\n",
    "\n",
    "from cooking_recipe_assistant.evaluation.retrievers import evaluate\n",
    "from cooking_recipe_assistant.evaluation.optimization import run_hyperopt\n",
    "from cooking_recipe_assistant.rags.retrievers.es_bm25 import es_bm25_query\n",
    "from cooking_recipe_assistant.rags.retrievers.es_hybrid import es_hybrid_query\n",
    "from cooking_recipe_assistant.rags.retrievers.es_hybrid_rrf import es_hybrid_rrf_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c079f5-bcf6-4039-8de2-93a65b2e39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Search\n",
    "ES_URL = \"http://localhost:9200\"\n",
    "INDEX_NAME = \"cooking-recipes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90dc422-78b5-4f54-8b17-c29cfa0b547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meals: {meals}\n",
      "title: {title}\n",
      "ingredients: {ingredients}\n",
      "summary: {summary}\n",
      "instructions: {text}\n",
      "tips: {tips}\n"
     ]
    }
   ],
   "source": [
    "ENTRY_TEMPLATE = read_text(f\"{PROMPTS_CONFIG_DIR}/en_entry_template.txt\")\n",
    "print(ENTRY_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea2953b-d8f5-48e5-9d8c-8439553c5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_CLIENT = Elasticsearch(hosts=[ES_URL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae451b7a-0905-4174-b803-2360483bd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "MINILM_EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "#NMNET_EMBEDDING_MODEL_NAME = 'all-mpnet-base-v2'\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5217bb7-c4f1-40f5-9466-e6206e87a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS = HuggingFaceEmbeddings(model_name=MINILM_EMBEDDING_MODEL_NAME)\n",
    "#EMBEDDINGS = HuggingFaceEmbeddings(model_name=NMNET_EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7df7a2f-0490-42af-a7ba-cae1e59a5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Crear el modelo OpenAI para el LLMChain\n",
    "OLLAMA_URL = 'http://localhost:11434'\n",
    "PLAYLIST_TITLE = 'Imperial Stout'\n",
    "OPENAI_MODEL_NAME = 'gpt-4o-mini'\n",
    "OLLAMA_MODEL_NAME = 'llama3'\n",
    "#llm = OpenAI(model=\"gpt-4\")\n",
    "#llm = ChatOllama(model=OLLAMA_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "738e5520-5aac-43af-ac74-14850bf28ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 68K\n",
      "-rw-rw-r-- 1 aztleclan aztleclan  109 oct 27 01:30 en_entry_template.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1,4K oct 18 17:24 en_prompt_template_blocks.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan  831 oct 26 14:11 en_prompt_template_eval_rag_v1.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan  643 oct 25 17:50 en_prompt_template_eval_rag_v2.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1,6K oct 18 20:04 en_prompt_template_extractions.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1,6K oct 25 15:44 en_prompt_template_ground_truth_v1.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1,5K oct 25 15:47 en_prompt_template_ground_truth_v2.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1,2K oct 23 18:16 en_prompt_template_questions.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan  206 oct 24 16:33 en_prompt_template_rag_v1.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan  206 oct 26 15:25 en_prompt_template_rag_v2.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan  346 oct 25 02:00 en_prompt_template_system_assistent.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1,5K oct 18 17:24 es_prompt_template_blocks.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1,7K oct 18 23:07 es_prompt_template_extractions_v1.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1,7K oct 18 22:59 es_prompt_template_extractions_v2.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1000 oct 16 01:09 prompt_template_divide_blocks.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 2,1K oct 18 17:21 prompt_template_transcript.txt\n",
      "-rw-rw-r-- 1 aztleclan aztleclan 1,8K oct 17 13:40 prompt_template_transcript_V2.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh \"{PROMPTS_CONFIG_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc3b230-0a79-47de-a008-644b55624444",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_COOKING_RECIPE_TEMPLATE = read_text(f\"{PROMPTS_CONFIG_DIR}/en_prompt_template_rag_v1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9b5e31-96f0-4e2c-a0ec-4e5b5ae247f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a cooking recipe asistente. Answer the QUESTION based on the CONTEXT of our recipe database. \n",
      "Use only the data in the CONTEXT when answering the QUESTION.\n",
      "\n",
      "CONTEXT: \n",
      "{context}\n",
      "\n",
      "QUESTION: {question}\n"
     ]
    }
   ],
   "source": [
    "print(BASIC_COOKING_RECIPE_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df90f33-897b-49c6-8255-8793d16aad91",
   "metadata": {},
   "source": [
    "# Check ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a51fd1b-610e-468a-aa5e-0813328d5d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"3cfc0904bf39\",\n",
      "    \"cluster_name\": \"docker-cluster\",\n",
      "    \"cluster_uuid\": \"wg43N1DqSdy9g9z_pOLIDQ\",\n",
      "    \"version\": {\n",
      "        \"number\": \"8.4.3\",\n",
      "        \"build_flavor\": \"default\",\n",
      "        \"build_type\": \"docker\",\n",
      "        \"build_hash\": \"42f05b9372a9a4a470db3b52817899b99a76ee73\",\n",
      "        \"build_date\": \"2022-10-04T07:17:24.662462378Z\",\n",
      "        \"build_snapshot\": false,\n",
      "        \"lucene_version\": \"9.3.0\",\n",
      "        \"minimum_wire_compatibility_version\": \"7.17.0\",\n",
      "        \"minimum_index_compatibility_version\": \"7.0.0\"\n",
      "    },\n",
      "    \"tagline\": \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "info_es = ES_CLIENT.info()\n",
    "print(json.dumps(info_es.body, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aa8b2b6-9efe-4583-b02e-11bb8187c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"cooking-recipes\": {\n",
      "        \"aliases\": {},\n",
      "        \"mappings\": {\n",
      "            \"properties\": {\n",
      "                \"chunk_number\": {\n",
      "                    \"type\": \"integer\"\n",
      "                },\n",
      "                \"doc_id\": {\n",
      "                    \"type\": \"text\",\n",
      "                    \"fields\": {\n",
      "                        \"keyword\": {\n",
      "                            \"type\": \"keyword\",\n",
      "                            \"ignore_above\": 256\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"id\": {\n",
      "                    \"type\": \"text\"\n",
      "                },\n",
      "                \"ingredients\": {\n",
      "                    \"type\": \"keyword\"\n",
      "                },\n",
      "                \"meals\": {\n",
      "                    \"type\": \"keyword\"\n",
      "                },\n",
      "                \"summart_vector\": {\n",
      "                    \"type\": \"float\"\n",
      "                },\n",
      "                \"summary\": {\n",
      "                    \"type\": \"text\"\n",
      "                },\n",
      "                \"summary_vector\": {\n",
      "                    \"type\": \"dense_vector\",\n",
      "                    \"dims\": 384,\n",
      "                    \"index\": true,\n",
      "                    \"similarity\": \"cosine\"\n",
      "                },\n",
      "                \"text\": {\n",
      "                    \"type\": \"text\"\n",
      "                },\n",
      "                \"text_vector\": {\n",
      "                    \"type\": \"dense_vector\",\n",
      "                    \"dims\": 384,\n",
      "                    \"index\": true,\n",
      "                    \"similarity\": \"cosine\"\n",
      "                },\n",
      "                \"tips\": {\n",
      "                    \"type\": \"keyword\"\n",
      "                },\n",
      "                \"title\": {\n",
      "                    \"type\": \"text\"\n",
      "                },\n",
      "                \"title_vector\": {\n",
      "                    \"type\": \"dense_vector\",\n",
      "                    \"dims\": 384,\n",
      "                    \"index\": true,\n",
      "                    \"similarity\": \"cosine\"\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"settings\": {\n",
      "            \"index\": {\n",
      "                \"routing\": {\n",
      "                    \"allocation\": {\n",
      "                        \"include\": {\n",
      "                            \"_tier_preference\": \"data_content\"\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"number_of_shards\": \"1\",\n",
      "                \"provided_name\": \"cooking-recipes\",\n",
      "                \"creation_date\": \"1729698947472\",\n",
      "                \"number_of_replicas\": \"0\",\n",
      "                \"uuid\": \"xRPfDileSMin8UmdVwEoOQ\",\n",
      "                \"version\": {\n",
      "                    \"created\": \"8040399\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if ES_CLIENT.indices.exists(index=INDEX_NAME):\n",
    "    info_indice = ES_CLIENT.indices.get(index=INDEX_NAME)\n",
    "    print(json.dumps(info_indice.body, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83f60a2c-870a-4ea2-a8e3-507822bf2dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"cooking-recipes\": {\n",
      "        \"settings\": {\n",
      "            \"index\": {\n",
      "                \"routing\": {\n",
      "                    \"allocation\": {\n",
      "                        \"include\": {\n",
      "                            \"_tier_preference\": \"data_content\"\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"number_of_shards\": \"1\",\n",
      "                \"provided_name\": \"cooking-recipes\",\n",
      "                \"creation_date\": \"1729698947472\",\n",
      "                \"number_of_replicas\": \"0\",\n",
      "                \"uuid\": \"xRPfDileSMin8UmdVwEoOQ\",\n",
      "                \"version\": {\n",
      "                    \"created\": \"8040399\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if ES_CLIENT.indices.exists(index=INDEX_NAME):\n",
    "    settings = ES_CLIENT.indices.get_settings(index=INDEX_NAME)\n",
    "    print(json.dumps(settings.body, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d578d2c-3275-4593-8cc5-1bf962de1009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count=232\n"
     ]
    }
   ],
   "source": [
    "if ES_CLIENT.indices.exists(index=INDEX_NAME):\n",
    "    count = ES_CLIENT.count(index=INDEX_NAME)['count']\n",
    "    print(f\"Count={count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f3d40-17f0-4df7-849b-506a7db96685",
   "metadata": {},
   "source": [
    "# Check Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60cb132e-b2bc-48e7-b692-b4595b150af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 384\n",
      "[-0.03306527063250542, -0.04929625988006592, 0.0011788202682510018, -0.052408862859010696, -0.037587061524391174, 0.025819718837738037, -0.03928518667817116, 0.05620156601071358, 0.0902889296412468, -0.052350059151649475]\n"
     ]
    }
   ],
   "source": [
    "text = \"LangChain is a framework for developing applications powered by language models.\"\n",
    "\n",
    "\n",
    "embedding_vector = EMBEDDINGS.embed_query(text)\n",
    "print(type(embedding_vector), len(embedding_vector))\n",
    "print(embedding_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82ea78-fb1c-4002-aa01-5ac6f5f794e1",
   "metadata": {},
   "source": [
    "# Simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f13d7aa1-038e-4016-ba29-d629fde35b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0694d690-2c78-4ba1-8191-dbbc99a7b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d282fd1e-74de-47c7-8cb7-04297103e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_retriever(\n",
    "    query: str\n",
    "):\n",
    "    #print(f\"query: {query}\")\n",
    "    search_results = es_bm25_query(\n",
    "        es_client=ES_CLIENT,\n",
    "        index_name=INDEX_NAME,\n",
    "        query=query,\n",
    "    )\n",
    "    #return search_results\n",
    "    context = build_recipe_context(search_results)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b0b4947-7cc6-41ac-a5d7-c8ccd77c6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para construir el contexto de los resultados de búsqueda\n",
    "def build_recipe_context(search_results):\n",
    "    context = \"\"\n",
    "    for doc in search_results:\n",
    "        context = context + ENTRY_TEMPLATE.format(**doc) + \"\\n\\n\"\n",
    "        #context = context + doc[\"text\"] + \"\\n\\n\"\n",
    "    #print(context)\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0988b2d9-03d3-4d48-b75d-ec20fb7a3c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llm(\n",
    "    model_name:str\n",
    "):\n",
    "    # Build llm\n",
    "    if model_name.startswith(\"llama\"):\n",
    "        llm = ChatOllama(model=model_name)\n",
    "    elif model_name.startswith(\"gpt\"):\n",
    "        llm = ChatOpenAI(model_name=model_name)\n",
    "    else:\n",
    "        raise Exception(f\"Not found model name: {model_name}\")\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b814d598-3103-4cbe-916c-cdd1385ffddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_chain(model_name:str, template:str):\n",
    "\n",
    "    # Build Prompt\n",
    "    #prompt_template = PromptTemplate(\n",
    "    #    input_variables=[\"question\", \"context\"],\n",
    "    #    template=template,\n",
    "    #)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    llm = build_llm(model_name)\n",
    "\n",
    "    #memory = ConversationBufferMemory(llm=llm, memory_key=\"chat_history\", return_messages=True, output_key='answer')\n",
    "\n",
    "    #qa = ConversationalRetrievalChain.from_llm(llm, retriever=retv , memory=memory,\n",
    "    #                                           return_source_documents=True)\n",
    "\n",
    "    qa = (\n",
    "        #RunnableLambda(es_retriever)\n",
    "        #| {\"context\": build_recipe_context, \"question\": RunnablePassthrough()}\n",
    "         {\"context\": es_retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5011813-9702-4cfb-8acc-81543417f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función RAG con LangChain\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    context = build_context(search_results)\n",
    "    print(type(query))\n",
    "    print(query)\n",
    "    print(type(context))\n",
    "    print(context)\n",
    "    \n",
    "    # Ejecutar el chain con los valores de entrada\n",
    "    answer = chain.invoke({\n",
    "        \"question\": query,\n",
    "        \"context\": context\n",
    "    })\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "150554f8-f31b-48a8-a51c-969ae0b41109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ground_truth = pd.read_csv(GROUND_TRUTH_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a66f11a7-cb8c-483a-8ab6-043d795eee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : What is the best way to cook the potatoes for the Potato and Ground Meat Pie, given that I need to leave the skin on?\n",
      "Doc_id: lBqnLPKRLuQ@000\n"
     ]
    }
   ],
   "source": [
    "row_id = 30\n",
    "query = df_ground_truth.iloc[row_id].question\n",
    "doc_id = df_ground_truth.iloc[row_id].id\n",
    "print(f\"Query : {query}\")\n",
    "print(f\"Doc_id: {doc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e38eae36-f48e-44d3-be4f-145670b8624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qa_ollama_chain = create_chain(OLLAMA_MODEL_NAME, BASIC_COOKING_RECIPE_TEMPLATE)\n",
    "qa_chain = create_chain(OPENAI_MODEL_NAME, BASIC_COOKING_RECIPE_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bfed634-d053-4b6b-a05f-d810d742c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_answers = qa_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87034ddf-6231-4cf5-9356-a9bd5cbfa700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Potato and Ground Meat Pie, the best way to cook the potatoes while leaving the skin on is to start by washing the potatoes thoroughly. Then, place them in a pot of water and bring it to a boil over high heat. Once boiling, reduce the heat to medium and cook for about 30 minutes, or until the potatoes are tender. To check for doneness, pierce them with a fork; if it goes in easily, they are ready. After cooking, drain the potatoes and let them cool before peeling.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62999fc7-c89d-4fc8-821e-945a781c7999",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9c50b6d-bd02-4205-8a05-61d186554090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : What are the exact cooking times for the Delicious Cabracho Cake for Christmas, specifically for the fish and onion preparation?\n",
      "Doc_id: vJ55fsr81yw@000\n"
     ]
    }
   ],
   "source": [
    "row_id = 20\n",
    "query = df_ground_truth.iloc[row_id].question\n",
    "doc_id = df_ground_truth.iloc[row_id].id\n",
    "print(f\"Query : {query}\")\n",
    "print(f\"Doc_id: {doc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9278b36-7085-4829-855c-b72765a59512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = rag_bm25(query, llm, template=BASIC_COOKING_RECIPE_TEMPLATE)\n",
    "q20_answer = qa_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e13f175-0582-4bb5-80b6-a591f8935c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Delicious Cabracho Cake for Christmas, the exact cooking times for the fish and onion preparation are as follows:\n",
      "\n",
      "- Cook the cabracho fish over medium heat for about **one and a half minutes on each side**.\n",
      "- Cook the onion over medium heat for about **10 to 15 minutes**.\n"
     ]
    }
   ],
   "source": [
    "print(q20_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf766d61-c2d7-47d1-8b5b-cbd30b69354a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
